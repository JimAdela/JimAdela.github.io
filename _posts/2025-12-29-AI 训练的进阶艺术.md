---
layout:     post
title:      AI 训练的进阶艺术：优化算法与过拟合防御
subtitle:   让神经网络学得更快、更稳、更通用
date:       2025-12-30
author:     WY
header-img: img/home-bg-art.jpg
catalog: true
tags:
    - 优化算法
    - Adam
    - 正则化
    - 批归一化
---

## 第十三章：优化算法——给 AI 下山穿上“溜冰鞋”

单纯的梯度下降在复杂的 AI 训练中往往不够快，或者容易卡在小坑里。

### 1. SGD (随机梯度下降)
*   **直观理解**：不再看完全部数据才走一步，而是看一小份（Batch）数据就走一步。
*   **优点**：计算快，且这种“抖动”能帮 AI 跳出一些小的局部陷阱。

### 2. Momentum (动量法)
*   **直观理解**：下山的铁球带有惯性。
*   **数学意义**：不仅考虑当下的梯度，还累加之前的运动方向。遇到平地时，靠惯性能冲过去，避免停在半山腰。

### 3. Adam (智能导航)
*   **核心逻辑**：AI 界的“自动挡”。它会自动调节每个参数的学习率——经常变动的参数步子小点，不怎么动的步子大点。它是目前最主流的选择。

---

## 第十四章：正则化——防御“死记硬背”

### 1. 过拟合 (Overfitting)
*   **现象**：训练集表现完美（100分），测试集表现糟糕（不及格）。
*   **本质**：模型记住了噪声和细节，却丢掉了普遍规律。

### 2. L2 正则化 (权重衰减)
*   **直观理解**：惩罚“出头鸟”。
*   **数学做法**：在误差函数（Loss）后面加一个惩罚项：$\lambda \sum W^2$。
*   **结果**：强迫权重 $W$ 尽可能变小，使模型变简单，从而提高通用性。

### 3. Dropout (随机罢工)
*   **直观理解**：训练时随机让一部分神经元“断开连接”。
*   **结果**：迫使剩下的神经元必须独立承担任务，不能产生依赖，增强了模型的鲁棒性。

---

## 第十五章：批归一化 (Batch Norm) —— 数据的“统一制服”

### 1. 为什么需要？
如果有的数据是 0-1000，有的是 0-1，矩阵运算会非常痛苦，梯度会变得极其不稳定。

### 2. 做法
在每一层计算完后，强行将数据拉回到**均值为 0，方差为 1** 的正态分布上。
*   **好处**：训练速度提升 10 倍，且对初始参数不那么敏感。

---